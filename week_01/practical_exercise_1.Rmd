---
title: "practical_exercise_1, Methods 3, 2021, autumn semester"
author: 'Louise Brix Pilegaard Hansen'
date: '20/9/2021'
output:
  html_document: default
  word_document: default
  pdf_document: default
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
setwd("~/Desktop/github_methods_3/week_01")
pacman::p_load("tidyverse")
```

## Dataset mtcars
Let's look at the "mtcars" data:  

_[, 1]   mpg   Miles/(US) gallon  
[, 2]	 cyl	 Number of cylinders  
[, 3]	 disp	 Displacement (cu.in.)  
[, 4]	 hp	 Gross horsepower  
[, 5]	 drat	 Rear axle ratio  
[, 6]	 wt	 Weight (lb/1000)  
[, 7]	 qsec	 1/4 mile time  
[, 8]	 vs	 V/S  
[, 9]	 am	 Transmission (0 = automatic, 1 = manual)  
[,10]	 gear	 Number of forward gears  
[,11]	 carb	 Number of carburetors_  


## Miles per gallon and weight

We can do a scatter plot, and it looks like there is some relation between fuel usage and the weight of cars.
Let's investigate this further

```{r,fig.height=5, fig.width=6}
par(font.lab=2, font.axis=2, cex=1.2)
plot(mpg ~ wt, data=mtcars, xlab='Weight (lb/1000)', ylab='Miles/(US) gallon',
     main='Scatter plot', ylim=c(0, 40))
```

# Exercises and objectives
The objectives of today's exercises are:  
1) To remind you of the (general) linear model, and how we can use it to make models in R  
2) To make some informal model comparisons  
3) To estimate models based on binomially distributed data  

If you would like to read more about a given function, just prepend the function with a question mark, e.g.  
``` {r}
?lm
```

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below   

## Exercise 1
The general linear model: $Y = X \beta + \epsilon$:  
Do a linear regression, expressing fuel usage as a function of weight using the function __lm__  
```{r}
data(mtcars)
model <- lm(mpg ~ wt, data=mtcars)
summary(model)
```
1. extract $\hat{\beta}$, $Y$, $\hat{Y}$, $X$ and $\epsilon$ from __model__ (hint: have a look at the function __model.matrix__)  

```{r}
beta_hat <- model$coefficients
Y_hat <- beta_hat[1] + mtcars$wt*beta_hat[2]
Y <- mtcars$mpg
X <- mtcars$wt
error <- model$residuals
```


  i. create a plot that illustrates $Y$ and $\hat{Y}$ (if you are feeling ambitious, also include $\epsilon$ (hint: you can use the function __arrows__))
    
```{r}
plot(X, Y)
abline(a = beta_hat[1], b = beta_hat[2], col = "blue")
arrows(x0=X, y0 = Y, y1 = Y_hat, length = 0, code = 2, angle = 0)
```

**- The blue line shows predicted values of Y (Y-hat) whereas the dots shows the observed values of Y. The lines are the error**


2. estimate $\beta$ for a quadratic model ($y = {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) using ordinary least squares _without_ using __lm__; $\hat{\beta} = {({X}^{T} X)}^{-1} {X}^{T} Y$ (hint: add a third column to $X$ from step 1)

```{r}
#creating a design matrix, where the first column contains 1's, the second contains the wt variable from mtcars, and the third columns is the wt-variable squared

n <- 32
const <- rep(1,n)
design_matrix <- cbind(const, mtcars$wt, (mtcars$wt)^2)

#using the OLS equation to calculate our beta values
beta_ols <- solve(t(design_matrix)%*%design_matrix)%*%t(design_matrix)%*%Y
beta_ols
```


3. compare your acquired $\hat{\beta}$ with the output of the corresponding quadratic model created using __lm__ (hint: use the function __I__, see details under help and the sub-section formula operators here: https://www.datacamp.com/community/tutorials/r-formula-tutorial)  

```{r}
design_matrix <- cbind(design_matrix, mtcars$mpg)
colnames(design_matrix) <- c("const", "wt", "wt_sqr", "mpg")

design_matrix <- data.frame(design_matrix)

lm_model <- lm(mpg ~ wt + wt_sqr, data = design_matrix)
summary(lm_model)
```

**- the values are the same for the manual OLS estimator and the lm function**

  i. create a plot that illustrates $Y$ and $\hat{Y}$ (if you are feeling ambitious, also include $\epsilon$ (hint: you can use the function __arrows__))  


```{r}
design_matrix <- as.matrix(design_matrix)
yhat <- design_matrix[,1:3]%*%beta_ols
```


```{r}
#linear plot
plot(Y, Y_hat)
abline(a = 0, b = 1, col = 'darkgreen')

#quadratic plot
plot(Y, yhat)
abline(a = 0, b = 1, col = 'darkgreen')
```


**I have plotted a line with slope 1 and intercept 1 to show the difference between the model predictions and the actual y-values. If the two values were the same, they would all be on the green line. **

## Exercise 2
Compare the plotted quadratic fit to the linear fit  

1. which seems better?  
**- the quadratic plot seems to fit the data a bit better that the linear fit. The errors are distributed more evenly across the entire model, whereas for the linear model, some of the errors are more extreme than others.**

2. calculate the sum of squared errors, (show the calculation based on $\epsilon$). Which fit has the lower sum?  

```{r}
# linear model
sum((Y - Y_hat)^2)

# quadratic fit
sum((Y - yhat)^2)
```
**- The quadratic fit has the lower sum of squared errors**

3. now make a cubic fit ($y = {\beta}_{3} x^3 + {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) and compare it to the quadratic fit 

```{r}
cub_df <- cbind(design_matrix[,1:3], (mtcars$wt)^3)
colnames(cub_df) <- c("const", "wt", "wt_sqr", "wt_cub")
cub_df <- cbind(cub_df, mtcars$mpg)
colnames(cub_df) <- c("const", "wt", "wt_sqr", "wt_cub", "mpg")

cub_df <- data.frame(cub_df)

cubic_model <- lm(mpg ~ wt + wt_sqr + wt_cub, data = cub_df)
summary(cubic_model)

cubic_betahat <- cubic_model$coefficients
```

  i. create a plot that illustrates $Y$ and $\hat{Y}$ for both the cubic and the quadratic fits (plot them in the same plot)  

```{r}
both_plot <- ggplot(mtcars, aes(x = wt, y = mpg))+
  geom_point()+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1, se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2) + I(x^3), size = 1, color = "green", se = FALSE)
both_plot
```

**- Blue line shows quadratic fit, green line shows cubic.**


  ii. compare the sum of squared errors 
```{r}
cub_df <- as.matrix(cub_df)

#calculating the y-hats for the cubic model
cubic_yhat <- cub_df[,1:4]%*%cubic_betahat

#comparing the quadratic fit and the cubic fit

# quadratic model
sum((Y - yhat)^2)

# cubic fit
sum((Y - cubic_yhat)^2)
```
**There is not very big difference - the cubic fit has a slightly smaller sum of squared errors.**


 iii. what's the estimated value of the "cubic" (${\beta}_3$) parameter? Comment on this!  
```{r}
cubic_betahat[4]
```
 **- The estimated value of the "cubic" parameter is 0.0459, which is quite small. It doesn't add much to our model.**

4. bonus question: which summary statistic is the fitted value (_Intercept_ or ${\beta}_0$ in $y = {\beta}_0$) below identical to?
```{r, echo=FALSE}
lm(mpg ~ 1, data=mtcars)
mean(mtcars$mpg)
```
**It's identical to the mean, since making a regression with only an intercept is the same as taking the mean of the dependent variable, as the slope is 0. **

## Exercise 3
Doing a logistic regression - estimating the probability that a car has automatic transmission (0) or manual transmission (1) based on its weight
```{r}
logistic.model <- glm(am ~ wt, data=mtcars, family='binomial')
summary(logistic.model)
```

Probabilities live on the range $(0, 1)$ - using the so-called logit function as a "link-function" we can map these onto the range $(-\infty, \infty)$, i.e. the real numbers.  
  
What we model in this case is: $Pr(y = 1) = logit^{-1}(X \beta)$, i.e. the probability of a car having manual transmission, given its weight. $X \beta$ is called the linear predictor; compare with $Y = X \beta + \epsilon$ 
It is helpful to define the logit function and its inverse function for the following:  

```{r}
logit <-     function(x) log(x / (1 - x))
inv.logit <- function(x) exp(x) / (1 + exp(x))
```

1. plot the fitted values for __logistic.model__:
```{r}
yhat_logistic <- inv.logit(predict(logistic.model))
logistic_df <- data.frame(cbind(mtcars$am, mtcars$wt, yhat_logistic))
colnames(logistic_df) <- c("am", "wt", "yhat_logistic")

ggplot(logistic_df, aes(x=wt, y=yhat_logistic))+ 
  geom_point()+
  xlab("wt")+
  ylab("P(Y = 1)")
```

  i. what is the relation between the __linear.predictors__ and the __fitted_values__ of the __logistic.model__ object?

The relationship between the linear predictors and the fitted values are not linear. This is why we can't interpret the model summary the same way as for a linear regression. This also means that the difference in y for each increment of x is **not** constant, as it is for a linear regression. This means that the estimates are hard to interpret. 

The estimates on their own are hard to interpret since the relationship between the independent variables and the dependent variables are not linear, and the estimates are on the log-scale. We can use their sign and size, but the numbers on their own are hard to interpret. Thus, we must use the inv.logit function in order to interpret them.

The fitted values predicted by our model are also hard to interpret on their own, and can therefore be transformed by the inv.logit function to be converted into probabilities, which are more easily interpreted. After the inv.logit transformation, the fitted values now express the probability of a car having a manual gear (P(Y = 1)) given the weight of the car.

2. plot the logistic function, you've estimated based on your $\hat{\beta}$, (not just the fitted values). Use an _xlim_ of (0, 7)

```{r}
ggplot(logistic_df, aes(x=wt, y=yhat_logistic))+ 
  geom_point(color="green")+
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial), color="green")+
  xlim(c(0,7))+
  xlab("wt")+
  ylab("P(Y = 1)")
```

  i. what's the interpretation of the estimated $\hat{\beta}_0$ (the _Intercept_)
```{r}
logistic.model$coefficients[1]

inv.logit(logistic.model$coefficients[1])
```
**The intercept is 12.04, which is not easily interpretable, since it's on the log-scale. By using the inv.logit function, the numbers are converted into probabilities, which tell us that there is a 99.99% probability that a car has manual transmission if the weight is 0 (which is just a hypothetical scenario, since a car can't have weight 0). **

  ii. calculate the estimated probability that the Pontiac Firebird has automatic transmission, given its weight

```{r}
inv.logit(logistic.model$coefficients[1] + logistic.model$coefficients[2]*3.845)
```
**- There is a 3,1% probability that the Pontiac Firebird has automatic transmission given its weight**

  iii. bonus question - plot the logistic function and highlight all the cars where we guessed wrongly, if we used the following "quantizer" function:
    
\begin{equation}
  transmission_{guess}=
  \begin{cases}
    1 (manual), & \text{if}\ PR(y = 1) ≥ 0.5 \\
    0 (automatic), & \text{otherwise}
  \end{cases}
\end{equation}    

```{r}
names <- rownames(mtcars)
logistic_df <- cbind(logistic_df, names)
pacman::p_load("ggrepel")

# creating a new column where the the predicted probabilites above 0.5 get label "1" and below get 0
logistic_df$predicted <- ifelse(yhat_logistic>0.5,1,0)

#plotting the logistic funtion and highlighting those were we guessed wrongly
mtcars_plot <- ggplot(logistic_df, aes(x = wt, y = am))+
  geom_point()+
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial), color="green")+
  geom_label_repel(aes(label=ifelse(am != predicted, names, '')), hjust=0,vjust=0)+
  xlim(c(0,7))+
  theme_classic()+
  xlab("wt")+
  ylab("Y")
mtcars_plot
```

3. plot quadratic fit alongside linear fit 
```{r}
mtcars$quadratic <- (mtcars$wt)^2

quadr_fit <- glm(am ~ wt + quadratic, data=mtcars, family='binomial')
summary(quadr_fit)

yhat_quadr <- inv.logit(predict(quadr_fit))
logistic_df <- data.frame(cbind(logistic_df, yhat_quadr))

ggplot(logistic_df, aes(x=wt, y=yhat_logistic))+ 
  geom_point(color = "blue")+
  geom_point(y=yhat_quadr, color = "red")+
  geom_jitter()+
  xlab("wt")+
  ylab("P(Y = 1)")
```

  i. judging visually, does adding a quadratic term make a difference?
**- Not a lot - the predicted probabilites are almost the same**

  ii. check the details in the help of the AIC function - which of the models provide the better fit according to the AIC values and the residual deviance respectively?

```{r}
summary(logistic.model)
#residual deviance is 19.176

summary(quadr_fit)
#residual deviance is 19.118

AIC(logistic.model, quadr_fit)
```

**- the logistic model has the lower AIC, thus providing the better fit. The residual deviance is a bit lower that the quadratic fit - but not a lot. This means that the quadratic term does not improve the model very much.**

  iii. in your own words, why might it be good to penalise a model like the quadratic model, we just fitted.

**It's a good idea to penalise models in order to avoid overfitting. If a model is overfitted, it's hard to generalize it, and thus it will perform badly on new data, it hasn't seen before (because it has been to 'specialized' on the original data). Models that get too complex are penalized more than simpler models, in order to take the risk of overfitting into account. **
