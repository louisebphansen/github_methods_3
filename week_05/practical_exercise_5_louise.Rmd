---
title: "practical_exercise_5, Methods 3, 2021, autumn semester"
author: "Louise Brix Pilegaard Hansen"
date: "27/10/2021"
output:
  html_document:
    df_print: paged
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/github_methods_3/week_05")
pacman::p_load("tidyverse", "gridExtra", "lme4", "dfoptim", "readbulk", "boot", "multcomp")
```

# Exercises and objectives
The objectives of the exercises of this assignment are based on: https://doi.org/10.1016/j.concog.2019.03.007  
  
4) Download and organise the data from experiment 1  
5) Use log-likelihood ratio tests to evaluate logistic regression models  
6) Test linear hypotheses  
7) Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  


REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is part 2 of Assignment 2 and will be part of your final portfolio


# EXERCISE 4 - Download and organise the data from experiment 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 1 (there should be 29).  
The data is associated with Experiment 1 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame - note that some of the subjects do not have the _seed_ variable. For these subjects, add this variable and make in _NA_ for all observations. (The _seed_ variable will not be part of the analysis and is not an experimental variable)  

```{r message=FALSE, warning=FALSE}
# reading in the experiment 1 data (missing seed values have NA values instead)
df <- read_bulk(directory = "~/Desktop/github_methods_3/week_05/experiment_1", stringsAsFactors = T)
```

  i. Factorise the variables that need factorising  

```{r}
# the stringsAsFactors = T argument made all characters as factors. 

# also making subject and pas factors
df$subject <- as.factor(df$subject)
df$pas <- as.factor(df$pas)
```


  ii. Remove the practice trials from the dataset (see the _trial.type_ variable)  
```{r}
exp <- df %>% 
  filter(trial.type == "experiment")
```

  iii. Create a _correct_ variable  

```{r}
exp$target.type <- as.character(exp$target.type)

exp$target.type <- replace(exp$target.type, exp$target.type == "even", "e")
exp$target.type <- replace(exp$target.type, exp$target.type == "odd", "o")
exp$correct <- ifelse(exp$target.type == exp$obj.resp, 1, 0)
```

  iv. Describe how the _target.contrast_ and _target.frames_ variables differ compared to the data from part 1 of this assignment  

```{r}
unique(exp$target.contrast)
unique(exp$target.frames)
```

The target.contrast variable is the same throughout the entire experiment - it has a value of 0.1. This was not the case in experiment 2 - here, the target.contrast values were adjusted for each participant. 

The target frames variable represent how long the stimuli was presented - it has 6 different values; 1 - 6. 1 target frame is 11.8 seconds, 2 target frames are 23.6 seconds etc. In experiment 2, this variable was not included. 

# EXERCISE 5 - Use log-likelihood ratio tests to evaluate logistic regression models

1) Do logistic regression - _correct_ as the dependent variable and _target.frames_ as the independent variable. (Make sure that you understand what _target.frames_ encode). Create two models - a pooled model and a partial-pooling model. The partial-pooling model should include a subject-specific intercept.  

```{r}
pool_model <- glm(correct ~ target.frames, family = "binomial", data = exp)
partpool_model <- glmer(correct ~ target.frames + (1|subject), family = "binomial", data = exp)
```


  i. the likelihood-function for logistic regression is: $L(p)={\displaystyle\prod_{i=1}^Np^{y_i}(1-p)^{(1-y_i)}}$ (Remember the probability mass function for the Bernoulli Distribution). Create a function that calculates the likelihood.  

```{r}
likelihood_fun <- function(model){
  yhat <- fitted.values(model)
  y <- exp$correct
  prod(yhat^(y)*(1-yhat)^(1-y))
}
```

  ii. the log-likelihood-function for logistic regression is: $l(p) = {\displaystyle\sum_{i=1}^N}[y_i\ln{p}+(1-y_i)\ln{(1-p)}$. Create a function that calculates the log-likelihood  

```{r}
likelihood_log_fun <- function(model){
  yhat <- fitted.values(model)
  y <- exp$correct
  sum(y*log(yhat)+(1-y)*log(1-yhat))
}
```


  iii. apply both functions to the pooling model you just created. Make sure that the log-likelihood matches what is returned from the _logLik_ function for the pooled model. Does the likelihood-function return a value that is surprising? Why is the log-likelihood preferable when working with computers with limited precision?  
  
```{r}
# comparing my function with the exponential of the logLik function
likelihood_fun(pool_model)
exp(logLik(pool_model))

# comparing my function with the logLik function
likelihood_log_fun(pool_model)
logLik(pool_model)
```

The likelihood function gives us 0 (which the exp of the logLik function also does). This is surprising since theoretically, you can't get 0 from the likelihood function. This is because our yhat can't be zero, since log(0) is not defined. In reality, the number is just so small that the computer can't estimate the precise number. This is because we are multiplying a lot of small numbers, which eventually will give us an extremely small number very close to zero. This is why it's better to use log-likelihood, which will give us negative values, since taking the log of numbers between 0-1 will give us negative values. Our computers can actually estimate these numbers more precisely, thus making it easier for us to contrast them. 

  
  iv. now show that the log-likelihood is a little off when applied to the partial pooling model - (the likelihood function is different for the multilevel function - see section 2.1 of https://www.researchgate.net/profile/Douglas-Bates/publication/2753537_Computational_Methods_for_Multilevel_Modelling/links/00b4953b4108d73427000000/Computational-Methods-for-Multilevel-Modelling.pdf if you are interested)  

```{r}
logLik(partpool_model)
likelihood_log_fun(partpool_model)
```

We can see the calculation is a little off - this is because the log-likelihood function is different for multilevel models (so the function we defined as likelihood_log_fun doesn't hold).

2) Use log-likelihood ratio tests to argue for the addition of predictor variables, start from the null model, `glm(correct ~ 1, 'binomial', data)`, then add subject-level intercepts, then add a group-level effect of _target.frames_ and finally add subject-level slopes for _target.frames_. Also assess whether or not a correlation between the subject-level slopes and the subject-level intercepts should be included. 

```{r}
m1 <- glm(correct ~ 1, family = 'binomial', data = exp)
m2 <- glmer(correct ~ 1 + (1|subject), family = 'binomial', data = exp)
m3 <- glmer(correct ~ target.frames + (1|subject), family = 'binomial', data = exp)
m4<- glmer(correct ~ target.frames + (1 + target.frames||subject), family = 'binomial', data = exp)
m5<- glmer(correct ~ target.frames + (1 + target.frames|subject), family = 'binomial', data = exp)
```

```{r}
anova(m4,m1,m2,m3,m5)
```

We can see that m5, which includes both a random slope and random intercept, has the highest loglik value, lowest deviance and has a significant p-value. This means that compared to the m3 or m4 model, we can reject the null-hypothesis (which says that the parameters we add are pure noise). Since there is a significant difference between the m3/m4 models and m5, this tells us that if the added parameters are just pure noise, the smaller deviance value for the m5 model would be very surprising (because of the small p-value). Comparing the m4 and the m5 model, we see that adding the correlation between subject-level slopes and the subject-level intercepts (which is included in our m5 model) improves the model compared to m4 (which does not include their correlation) - this indicates that including the correlation is relevant for our model. 

  i. write a short methods section and a results section where you indicate which model you chose and the statistics relevant for that choice. Include a plot of the estimated group-level function with `xlim=c(0, 8)` that includes the estimated subject-specific functions.

```{r message=FALSE}
exp %>% ggplot()+
  geom_smooth(aes(x = target.frames, y = fitted.values(pool_model), color = "Complete pooling model"))+
  geom_smooth(aes(x = target.frames, y = fitted.values(m5), color = "Partial pooling model"))+
  facet_wrap(~subject)+
  xlim(c(0, 8))+
  ylim(c(0,1))+
  xlab("Target Frames")+
  ylab("Fitted values")+
  ggtitle("Estimated group-level functions with subject-specific functions")
```

**Methods**
A null model was constructed followed by four models of increasing complexity, including multilevel models with random slopes and intercepts. The models were compared using the anova() function. Here, we looked at the loglik values, deviance and the p-values of the five constructed models. 

**Results**
We chose our m5 model, which is as following: 

correct ~ target.frames + (1 + target.frames | subject)

The model was significant, with a logLIk = -10449, deviance = 20898 and chisqr(1) = 22.926 and p < 0.5. 

This was chosen since it has the highest loglik value, lowest deviance and has a significant p-value, and thus seemed the better choice compared to the other, more simpler models, since the added complexity did not seem to be pure noise. 


  ii. also include in the results section whether the fit didn't look good for any of the subjects. If so, identify those subjects in the report, and judge (no statistical test) whether their performance (accuracy) differed from that of the other subjects. Was their performance better than chance? (Use a statistical test this time) (50 %)  


The fit for subject 24 does not look too good. From the plot we see that the complete pooling model differs a lot from the partial pooling model for subject 24, which it doesn't for the other subjects. 


```{r}
subject_24 <- exp %>% 
  filter(subject == "24")

t.test(subject_24$correct, mu = 0.5)
```
By taking a one-sample t-test, we can see whether subject 24's performance is statistically better than chance. We see that subject 24's performance is significantly different from chance level. However, we see that the mean accuracy is 56,7% - which is still not very good. 

3) Now add _pas_ to the group-level effects - if a log-likelihood ratio test justifies this, also add the interaction between _pas_ and _target.frames_ and check whether a log-likelihood ratio test justifies this  
    i. if your model doesn't converge, try a different optimizer  

```{r}
m6<- glmer(correct ~ target.frames + pas + (1 + target.frames|subject), family = 'binomial', data = exp)

m7 <- glmer(correct ~ target.frames*pas + (1 + target.frames|subject), family = 'binomial', data = exp)

# comparing the three models
anova(m5, m6, m7)
```

A log-likehood ratio test justifies adding the interaction between pas and target.frames, since we see the p-value is significant for the interaction model (m7). 

  ii. plot the estimated group-level functions over `xlim=c(0, 8)` for each of the four PAS-ratings - add this plot to your report (see: 5.2.i) and add a description of your chosen model. Describe how _pas_ affects accuracy together with target duration if at all. 
  
Also comment on the estimated functions' behaviour at target.frame=0 - is that behaviour reasonable?  

```{r}
group_level <- glm(correct ~ target.frames*pas, family = "binomial", data = exp)

exp %>% ggplot(aes(x = target.frames, y = fitted.values(group_level)))+
  geom_line(aes(color = pas))+
  xlim(c(1, 6))+
  ylim(c(0,1))+
  xlab("Target Frames")+
  ylab("Probabilities")+
  ggtitle("Estimated group-level functions for each of the four PAS-ratings")+
  theme_minimal()
```

From running the anova, we see that adding the interaction between pas and target frames still give us a significant model (meaning that the p-value would be surprising if the added parameters were just pure noise) - this is hence our chosen model: 

correct ~ target.frames * pas + (1 + target.frames | subject)

The model includes an interaction, which means we expect that target.frames effects accuracy differently depending on the pas-rating. Furthermore, we allow varying subject-level intercepts and subject-level slopes for target.frames, hence making it a multilevel model. 

From the plot above, we see that pas level has an effect on the influence of target frames on accuracy, as we see the four lines are slightly different. This can also be seen from the coefficients of the model; 

```{r}
summary(m7)$coefficients
```

From the coefficients we see that only pas4 is not a significant predictor. We see that the estimates for target.frames:pas2 and target.frames:pas3 are quite different  - this means that target frames seem to have different effects on accuracy in the different pas levels, making it very relevant to include the interaction between the two variables in our model. 

Furthermore, we see than target.frames on its own has a positive estimate, meaning than longer duration of stimuli seems to increase accuracy, even for the pas 1 rating. If we look at the interaction coefficients, we see that these are all positive - this means that for higher PAS ratings, we see an increase in accuracy with an increase in target.frames.
 

```{r}
# commenting on the estimated functions' behaviour at target.frame = 0. extracting the intercept - since this is target frames= 0 and pas = 1 
inv.logit(m7@beta[1])
```

By taking the inverse logit of the intercept (since this is target frames at 0 and the other variables held constant), we can find the accuracy score when pas = 1 and target.frames = 0.
Since target frames being 0 would theoretically be not seeing the cue, we would expect accuracy to be at chance level. However, this is not the case, as we see that the accuracy score is 46,9%, making it lower than chance level. 


# EXERCISE 6 - Test linear hypotheses

In this section we are going to test different hypotheses. We assume that we have already proved that more objective evidence (longer duration of stimuli) is sufficient to increase accuracy in and of itself and that more subjective evidence (higher PAS ratings) is also sufficient to increase accuracy in and of itself.  
We want to test a hypothesis for each of the three neighbouring differences in PAS, i.e. the difference between 2 and 1, the difference between 3 and 2 and the difference between 4 and 3. More specifically, we want to test the hypothesis that accuracy increases faster with objective evidence if subjective evidence is higher at the same time, i.e. we want to test for an interaction.  

1) Fit a model based on the following formula: `correct ~ pas * target.frames + (target.frames | subject))`

```{r}
# the model m7 is the interaction model, correct ~ target.frames * pas + (1 + target.frames | subject)
```

  i. First, use `summary` (yes, you are allowed to!) to argue that accuracy increases faster with objective evidence for PAS 2 than for PAS 1. 
  
```{r}
summary(m7)
```
  
The slope for the interaction between target.frames:pas2 is positive, which means that compared to pas1, accuracy increases faster with objective evidence for pas2.

2) `summary` won't allow you to test whether accuracy increases faster with objective evidence for PAS 3 than for PAS 2 (unless you use `relevel`, which you are not allowed to in this exercise). Instead, we'll be using the function `glht` from the `multcomp` package

  i. To redo the test in 6.1.i, you can create a _contrast_ vector. This vector will have the length of the number of estimated group-level effects and any specific contrast you can think of can be specified using this. For redoing the test from 6.1.i, the code snippet below will do
  
```{r}
## testing whether PAS 2 is different from PAS 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh <- glht(m7, contrast.vector)
print(summary(gh))

## as another example, we could also test whether there is a difference in
## intercepts between PAS 2 and PAS 3
contrast.vector <- matrix(c(0, -1, 1, 0, 0, 0, 0, 0), nrow=1)
gh <- glht(m7, contrast.vector)
print(summary(gh))
```
  

  ii. Now test the hypothesis that accuracy increases faster with objective evidence for PAS 3 than for PAS 2.

```{r, eval=FALSE}
contrast.vector2 <- matrix(c(0, 0, 0, 0, 0, -1, 1, 0), nrow=1)
gh2 <- glht(m7, contrast.vector2)
print(summary(gh2))
```

We get a significant p-value, so accuracy increases faster with more objective evidence (increasing values of target.frames) for pas 3 than for pas 2. 

  iii. Also test the hypothesis that accuracy increases faster with objective evidence for PAS 4 than for PAS 3
  
```{r}
contrast.vector3 <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1)
gh3 <- glht(m7, contrast.vector3)
print(summary(gh3))
```
  
No significant p-value - the slope (estimate) is positive, which means that accuracy increases faster for pas4 than for pas3 with more objective evidence, but that this difference is not significant. 
  
3) Finally, test that whether the difference between PAS 2 and 1 (tested in 6.1.i) is greater than the difference between PAS 4 and 3 (tested in 6.2.iii)

```{r}
# difference between pas2 and pas1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh <- glht(m7, contrast.vector)

# the difference between pas3 and pas4
contrast.vector3 <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1)

# finding the difference of differences
K <- (contrast.vector - contrast.vector3)
t <- glht(m7, linfct = K)
summary(t)
```
Significantly bigger difference between pas 2 and 1 than between pas4 and pas3, meaning there is a significant difference between the two differences. 

# EXERCISE 7 - Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

We saw in 5.3 that the estimated functions went below chance at a target duration of 0 frames (0 ms). This does not seem reasonable, so we will be trying a different approach for fitting here.  
We will fit the following function that results in a sigmoid, $f(x) = a + \frac {b - a} {1 + e^{\frac {c-x} {d}}}$  

It has four parameters: _a_, which can be interpreted as the minimum accuracy level, _b_, which can be interpreted as the maximum accuracy level, _c_, which can be interpreted as the so-called inflexion point, i.e. where the derivative of the sigmoid reaches its maximum and _d_, which can be interpreted as the steepness at the inflexion point. (When _d_ goes towards infinity, the slope goes towards a straight line, and when it goes towards 0, the slope goes towards a step function).  
  
We can define a function of a residual sum of squares as below

```{r}
RSS <- function(dataset, par)
{
    ## "dataset" should be a data.frame containing the variables x (target.frames)
    ## and y (correct)
    
    ## "par" are our four parameters (a numeric vector) 
    ## par[1]=a, par[2]=b, par[3]=c, par[4]=d
  
  a = par[1]
  b = par[2]
  c = par[3]
  d = par[4]
  
    x <- dataset$x
    y <- dataset$y
    y.hat <- a + ((b-a)/(1+exp((c-x)/d)))
    RSS <- sum((y - y.hat)^2)
    return(RSS)
}
```

1) Now, we will fit the sigmoid for the four PAS ratings for Subject 7
    i. use the function `optim`. It returns a list that among other things contains the four estimated parameters. You should set the following arguments:  
    `par`: you can set _c_ and _d_ as 1. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `fn`: which function to minimise?  
    `data`: the data frame with _x_, _target.frames_, and _y_, _correct_ in it  
    `method`: 'L-BFGS-B'  
    `lower`: lower bounds for the four parameters, (the lowest value they can take), you can set _c_ and _d_ as `-Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `upper`: upper bounds for the four parameters, (the highest value they can take) can set _c_ and _d_ as `Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)    
    

a and b in 'par': we set a, the minimum accuracy level, as 0.5, as we want it to minimum at chance level. b, the maximum accuracy level' is set as 1, since you can't be more than 100% accurate.

a and b in 'lower': both a and b are set as 0.5, since we still want it to be at minimum chance level.

a and b in 'upper': both a and b are set as 1, since you can't get an accuracy score above 100%.

```{r}
# creating a dataframe with only subject 7, target frames, correct and pas
subject7 <- exp %>% 
  dplyr::filter(subject == "7") %>% 
  dplyr::select('x' = "target.frames",'y' = "correct", "pas")

# running the optim function on each of the four pas ratings

pas1 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS,
      data = filter(subject7, pas == "1"),
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf))

pas2 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS,
      data = filter(subject7, pas == "2"),
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf))

pas3 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS,
      data = filter(subject7, pas == "3"),
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf))

pas4 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS,
      data = filter(subject7, pas == "4"),
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf))
```

  ii. Plot the fits for the PAS ratings on a single plot (for subject 7) `xlim=c(0, 8)`

```{r}
# creating a function for the sigmoid function
sigmoid <- function(a,b,c,d,x){
  y.hat <- a + ((b-a)/(1+exp((c-x)/d)))
  return(y.hat)
}

# simulating x-values between 0-8 with increments of 0.01 in order to make a nice geom_line for the plot
x_values <- seq(0, 8, by = 0.01)

# calculating yhat values for each of the four pas ratings using the sigmoid function
yhat1 <- sigmoid(pas1$par[1], pas1$par[2], pas1$par[3], pas1$par[4], x_values)
yhat2 <- sigmoid(pas2$par[1], pas2$par[2], pas2$par[3], pas2$par[4], x_values)
yhat3 <- sigmoid(pas3$par[1], pas3$par[2], pas3$par[3], pas3$par[4], x_values)
yhat4 <- sigmoid(pas4$par[1], pas4$par[2], pas4$par[3], pas4$par[4], x_values)

# combining it all into a dataframe
values <- data.frame(x_values, yhat1, yhat2, yhat3, yhat4)

# plotting the four lines for each of the pas ratings
ggplot(values, aes(x = x_values))+
  geom_line(aes(y = yhat1, color = "blue"))+
  geom_line(aes(y = yhat2, color = "green"))+
  geom_line(aes(y = yhat3, color = "red"))+
  geom_line(aes(y = yhat4, color = "orange"))+
  scale_color_discrete(name="Pas",
                         breaks=c("blue", "green", "red", "orange"),
                         labels=c("Pas 1", "Pas 2", "Pas 3", "Pas 4 "))+
  xlim(c(0, 8))+
  ylim(c(0,1))+
  xlab("Target Frames")+
  ylab("Predicted Accuracy")+
  ggtitle("Sigmoid-fits for each PAS rating")
```

  iii. Create a similar plot for the PAS ratings on a single plot (for subject 7), but this time based on the model from 6.1 `xlim=c(0, 8)`   
  
```{r warning=FALSE}
# creating hypothetical values for target frames and the four pas values, again in order to create nice lines for the plot - these will be our x-values that our model will predict yhat values from, which we then will be able to plot
newdat <- data.frame(cbind('target.frames' = seq(0, 8, by = 0.001), 'pas' = rep(1:4), 'subject' = rep('7')))

# making the variables the appropriate classes
newdat$subject <- as.factor(newdat$subject)
newdat$pas <- as.factor(newdat$pas)
newdat$target.frames <- as.numeric(newdat$target.frames)

# predicting yhat values using the new dataframe
newdat$yhat <- predict(m7, newdata = newdat, type = "response")

# plotting
ggplot(newdat) + 
  geom_line(aes(x = target.frames, y = yhat, color = pas)) + 
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  theme_minimal()+
  ggtitle("Fits for each pas-rating from the interaction model")
```
  
  iv. Comment on the differences between the fits - mention some advantages and disadvantages of each way  

First of all, we see that in the interaction model, we have accuracy levels below chance (50%) at target.frames = 0. This is accounted for in the sigmoid fit, since we restricted the minimum accuracy level to be 50%, which makes more sense, since there would be no reason to be *worse* than chance level when presented with no stimuli (target.frames = 0). This is an advantage of the sigmoid model. 

One difference can be seen in the fit for pas1 (the red line). In the sigmoid fit, we see that it's estimated to be at chance through all the different target frame possibilities. In the interaction model, we see that the fit is not just at 50%, but is slowly increasing. One could imagine that subject 7 simply performed below chance level for the pas1 ratings - but since we have restricted the sigmoid function to have a minimum accuracy of 50%, this creates a "floor-effect", thus creating a straight line with 50% accuracy for all the target-frame levels.

Furthermore, there seems to be a difference for the pas 2 line in the two plots. For the interaction model plot, the pas 2 line (the green line) seems to increase more rapidly than it does in the sigmoid plot (also seen as a green line). Thus, it seems as though the interaction model expects accuracy to be effected by the interaction between pas2 and target.frames, meaning that it expects higher accuracy scores for pas2 when we have higher values of target.frames than the sigmoid function does. 

Finally, one could argue that it could be a disadvantage for the sigmoid function that the rate at which it increases seem to decrease at certain levels, creating a "flat" line from certain values and forward. For example, for the pas 3 line (the purple in the sigmoid plot), we see a big difference of accuracy scores between target.frames = 1 and target.frames = 3 - but not a visually detectable difference between target.frames levels of 3 and 4. In the interaction plot (pas3 is represented by the blue line), we can follow this difference more "smoothly", as we can see different accuracy levels between 3 and 4, for example. 

2) Finally, estimate the parameters for all subjects and each of their four PAS ratings. Then plot the estimated function at the group-level by taking the mean for each of the four parameters, _a_, _b_, _c_ and _d_ across subjects. A function should be estimated for each PAS-rating (it should look somewhat similar to Fig. 3 from the article:  https://doi.org/10.1016/j.concog.2019.03.007)
    i. compare with the figure you made in 5.3.ii and comment on the differences between the fits - mention some advantages and disadvantages of both.
    

```{r}
# creating a new df which includes the columns we need to run the optim function
subjects_df <- exp %>% 
  dplyr::select('x' = "target.frames",'y' = "correct", "pas", "subject")

# creating an empty dataframe for the output from the loop
output_df <- data.frame()

# creating a for-loop that iterates over each subject in our dataset
for (n in 1:length(unique(subjects_df$subject))){
  
  # creating another loop which runs the optim function for each of the four pas ratings
  for (i in 1:4){
    
    #creating a dataframe for each pas rating for each subject
    subject_pas_df <- subjects_df %>% 
      filter(subject == n & pas == i)
    
    parameters <- optim(
      par = c(0.5, 1, 1, 1),
      fn = RSS,
      data = subject_pas_df,
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf))
    
    loop_df <- data.frame(
      subjects = n,
      pas = i,
      a = parameters$par[1],
      b = parameters$par[2],
      c = parameters$par[3],
      d = parameters$par[4])
    
    output_df <- rbind(output_df, loop_df)
  }
}
```


```{r}
# averaging the parameters from our loop
average_parameters <- output_df %>% 
  group_by(pas) %>% 
  summarise(mean_a = mean(a), mean_b = mean(b), mean_c = mean(c), mean_d = mean(d))
```

```{r}
# creating four variables, which saves our functions, to be used in the geom_function in the following plot

sigmoid1 <- function(x) average_parameters$mean_a[1] + ((average_parameters$mean_b[1]-average_parameters$mean_a[1])/(1+exp((average_parameters$mean_c[1]-x)/average_parameters$mean_d[1])))

sigmoid2 <- function(x) average_parameters$mean_a[2] + ((average_parameters$mean_b[2]-average_parameters$mean_a[2])/(1+exp((average_parameters$mean_c[2]-x)/average_parameters$mean_d[2])))

sigmoid3 <- function(x) average_parameters$mean_a[3] + ((average_parameters$mean_b[3]-average_parameters$mean_a[3])/(1+exp((average_parameters$mean_c[3]-x)/average_parameters$mean_d[3])))

sigmoid4 <- function(x) average_parameters$mean_a[4] + ((average_parameters$mean_b[4]-average_parameters$mean_a[4])/(1+exp((average_parameters$mean_c[4]-x)/average_parameters$mean_d[4])))
```


```{r}
# plotting
ggplot()+
  geom_function(aes(colour = "pas1"), fun = sigmoid1) +
  geom_function(aes(colour = "pas2"),fun = sigmoid2) +
  geom_function(aes(colour = "pas3"),fun = sigmoid3) +
  geom_function(aes(colour = "pas4"),fun = sigmoid4) +
  scale_color_discrete(name="PAS",
                         breaks=c("pas1", "pas2", "pas3", "pas4"),
                         labels=c("Pas 1", "Pas 2", "Pas 3", "Pas 4 "))+
  xlim(c(0, 8))+
  ylim(c(0,1))+
  xlab("Target Frames")+
  ylab("Predicted Accuracy")+
  theme_minimal()
```
 i. compare with the figure you made in 5.3.ii and comment on the differences between the fits - mention some advantages and disadvantages of both.
 
```{r}
# remembering the plot from 5.3.ii :) 

exp %>% ggplot(aes(x = target.frames, y = fitted.values(group_level)))+
  geom_line(aes(color = pas))+
  xlim(c(1, 6))+
  ylim(c(0,1))+
  xlab("Target Frames")+
  ylab("Probabilities")+
  ggtitle("Estimated group-level functions for each of the four PAS-ratings")+
  theme_minimal()
```
The overall tendencies look similar; for example, for the pas1 rating, the estimated accuracy does not improve when the participants are shown more target frames - this is seen in both the sigmoid functions and the functions from the logistic interaction model. This could indicate that when the subjective experience is pas = 1, the participants are simply not better than chance regarding accuracy.

One advantage of the psychometric functions is that visually, it's easy to estimate where a 'change' in accuracy happens in relation to the stimulus level (amount of target frames) - this is because we can see a steep increase in the functions, which means the accuracy levels go up a lot for certain amounts of target frames (across the different pas ratings, indicated by the different colored lines). One disadvantage, and thus an advantage of the interaction model, is however that with higher values of target frames, there is visually no difference in accuracy ratings for the sigmoid functions (but there is for the interaction model fits). The interaction model predicts differences in accuracy scores across the different target frames, almost reaching 100% accuracy for pas 3 and pas 4 for 6 (the highest amount) target frames. In the sigmoid functions, the functions stays *almost* at a constant level after 4 target frames. This could again both be an advantage and a disadvantage - perhaps it's not very realistic that accuracy is predicted to be almost at 100% for pas3 and pas4 ratings for 6 target frames - hence the sigmoid function would be more appropriate, since this indicates that maximum accuracy flattens out at approximately 90% at target.frames = 4. However, since the task is arguably not the most difficult one, especially not when it's visible at 6 target.frames, one could perhaps expect that a large subjective experience of perceptual awareness (a high pas rating) would correctly predict accuracy, thus making it possible to reach 100% accuracy for the higher pas ratings. 

