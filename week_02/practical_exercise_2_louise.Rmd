---
title: "practical_exercise_2, Methods 3, 2021, autumn semester"
author: "Louise Brix Pilegaard Hansen"
date: "22/9/2021"
output:
  html_document:
    df_print: paged
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/github_methods_3/week_02")
library('lme4', 'tidyverse')
pacman::p_load("tidyverse")
```

# Assignment 1: Using mixed effects modelling to model hierarchical data
In this assignment we will be investigating the _politeness_ dataset of Winter and Grawunder (2012) and apply basic methods of multilevel modelling. 

## Dataset
The dataset has been shared on GitHub, so make sure that the csv-file is on your current path. Otherwise you can supply the full path.

```{r}
politeness <- read.csv('politeness.csv') ## read in data
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Learning to recognize hierarchical structures within datasets and describing them  
2) Creating simple multilevel models and assessing their fitness  
3) Write up a report about the findings of the study  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1 - describing the dataset and making some initial plots

1) Describe the dataset, such that someone who happened upon this dataset could understand the variables and what they contain 

**Subject**: Subject refers to the participant in the study

**Gender**: Gender of the participant - Male/Female 

**Scenario**: Scenario is the task they are asked to do. There are 7 different scenarios

**Attitude**: Either formal or informal. The participants were asked to do each scenario in a formal and informal way. 

**Total duration**: for how much time the participants are talking

**f0mn**: Frequensy in Hertz

**hiss_count**: The amount of "noisy breath intakes" during the response


  i. Also consider whether any of the variables in _politeness_ should be encoded as factors or have the factor encoding removed. Hint: ```?factor```  
  
```{r}
#Making all the variables that are characters to factors
politeness[sapply(politeness, is.character)] <- lapply(politeness[sapply(politeness, is.character)], 
                                       as.factor)
```

2) Create a new data frame that just contains the subject _F1_ and run two linear models; one that expresses _f0mn_ as dependent on _scenario_ as an integer; and one that expresses _f0mn_ as dependent on _scenario_ encoded as a factor  

```{r}
#filtering the dataframe so we get a dataframe with only F1
f1 <- politeness %>% 
  filter(subject == "F1")

#making the lm model where scenario is an integer
f1model_1 <- lm(f0mn ~ scenario, data = f1)
summary(f1model_1)
```
```{r}
#making scenario a factor and fitting a new model
f1$scenario <- as.factor(f1$scenario)
f1model_2 <- lm(f0mn ~ scenario, data = f1)
summary(f1model_2)
```

  i. Include the model matrices, $X$ from the General Linear Model, for these two models in your report and describe the different interpretations of _scenario_ that these entail

```{r}
model.matrix(f1model_1)
model.matrix(f1model_2)
```

We see that the design matrix for the two models look completely different. In the model where scenario is an integer, we only have one predictor, scenario. The interpretation thus assumes that for each increment of scenario-number, there is a linear relationship to the outcome, the pitch frequency. 
In the model where scenario is a factor, we get 6 different slopes and an intercept, where the intercept is the mean of the frequency for the first scenario, and the slopes are the differences from the intercept. So if you subtract the slope of each of the scenarios from the intercept, you get the mean frequency from that group. 

  ii. Which coding of _scenario_, as a factor or not, is more fitting?

The coding of scenario as a factor is more fitting. Treating it as an integer would be treating it as an ordinal variable, and we cannot say that scenario 4 is twice as much as scenario 2. Scenarios are nominal, and thus all need to weigh as much as each other - this means that you should code scenario as a factor. 

```{r}
#making scenario a factor
politeness$scenario <- as.factor(politeness$scenario)
```

3) Make a plot that includes a subplot for each subject that has _scenario_ on the x-axis and _f0mn_ on the y-axis and where points are colour coded according to _attitude_
    i. Describe the differences between subjects

```{r}
ggplot(politeness, aes(x = scenario, y = f0mn, color = attitude))+
  geom_point()+
  facet_wrap(~subject)+
  theme_minimal()+
  ylab("Frequency in Hz")+
  ggtitle("Plot for each subject")
```


Generally, we see that all the "M" participants, which are the men, have lower frequencies than the "W" participants, which are women. It looks like the influence of attitude has different effects for the different subjects

## Exercise 2  - comparison of models

For this part, make sure to have `lme4` installed.  
You can install it using `install.packages("lme4")` and load it using `library(lme4)`  
`lmer` is used for multilevel modelling

1) Build four models and do some comparisons
    i. a single level model that models _f0mn_ as dependent on _gender_
```{r}
gender_model <- lm(f0mn ~ gender, data = politeness)
summary(gender_model)
```

  ii. a two-level model that adds a second level on top of i. where unique intercepts are modelled for each _scenario_
  
```{r}
lmer_gender_model_scenario <- lmer(f0mn ~ gender + (1|scenario), data = politeness)
summary(lmer_gender_model_scenario)
```

  iii. a two-level model that only has _subject_ as an intercept 
```{r}
lmer_gender_model_subject <- lmer(f0mn ~ gender + (1|subject), data = politeness)
summary(lmer_gender_model_subject)
```

  iv. a two-level model that models intercepts for both _scenario_ and _subject_

```{r}
lmer_gender_model_both <- lmer(f0mn ~ gender + (1|subject) + (1|scenario), data = politeness)
summary(lmer_gender_model_both)
```

  v. which of the models has the lowest residual standard deviation, also compare the Akaike Information Criterion `AIC`?

```{r}
#finding the residual standard deviation and combining into a tibble for better overview
models <- c("gender_model", "lmer_gender_model_scenario", "lmer_gender_model_subject", "lmer_gender_model_both")

sigmas <- c(sigma(gender_model), sigma(lmer_gender_model_scenario), sigma(lmer_gender_model_subject), sigma(lmer_gender_model_both))

as_tibble(rbind(models, sigmas))
```

The model with both random intercepts for subject and scenario has the lowest residual standard deviation.

```{r}
#finding the AIC value
AIC(gender_model, lmer_gender_model_scenario, lmer_gender_model_subject, lmer_gender_model_both)
```

Again, the model with both random intercepts for subject and scenario has the lowest AIC value. 

  vi. which of the second-level effects explains the most variance?

We see that the model with subject as a second-level effect (above called "lmer_gender_model_subject") explains more variance than the one with scenario as a second-level effect (above called "lmer_gender_model_scenario"), as it both has a lower residual standard deviation and AIC value. Furthermore, we can also see that adding both scenario and subject as random intercepts (above seen in the model "lmer_gender_model_both") does not improve the model by a lot compared to just having subject as a random intercept. Thus, the second level effect "subject" explains the most variance.

2) Why is our single-level model bad?

It does not take individual differences into account - we have seen that adding subject as a random intercept explains a lot more variance than not adding it, thus underlining that there is a tendency in the data that the single-level model does not account for, but that the mixed effect models do. 

  i. create a new data frame that has three variables, _subject_, _gender_ and _f0mn_, where _f0mn_ is the average of all responses of each subject, i.e. averaging across _attitude_ and_scenario_

```{r}
#making a new dataframe with the variables and removing NA values from the original dataset so the mean can be calculated
average_df <- politeness %>% 
  na.omit() %>% 
  group_by(subject, gender) %>% 
  summarise(f0mn = mean(f0mn))
```

  ii. build a single-level model that models _f0mn_ as dependent on _gender_ using this new dataset

```{r}
avg_model <- lm(f0mn ~ gender, data = average_df)
summary(avg_model)
```

  iii. make Quantile-Quantile plots, comparing theoretical quantiles to the sample quantiles) using `qqnorm` and `qqline` for the new single-level model and compare it to the old single-level model (from 1).i). Which model's residuals ($\epsilon$) fulfil the assumptions of the General Linear Model better?)

```{r}
qqnorm(resid(gender_model), main = "QQ plot for the residuals of the model of frequency predicted by gender")
qqline(resid(gender_model))

qqnorm(resid(avg_model), main = "QQ plot for the residuals of the model of avg. frequency predicted by gender")
qqline(resid(avg_model))
```


I would argue that the second model, shown in the second plot, fulfills the assumption of normality of the residuals of the general linear model better. This is because the errors look more systematically distributed in the first plot; it appears as though there is a small curve and pattern in the first model. Furthermore, it seems as though more of the errors are distributed above the line, whereas in the second plot, they are more evenly distributed both above and under.

  iv. Also make a quantile-quantile plot for the residuals of the multilevel model with two intercepts. Does it look alright?

```{r}
qqnorm(resid(lmer_gender_model_both), main= "Quantile Quantile plot for the residuals of the multilevel model")
qqline(resid(lmer_gender_model_both))
```


It looks all right - there is not as much of a pattern as we saw in the first plot above. The errors are more evenly distributed above and under the line in this plot - but there are still some deviations from the line, which we can see to the far right of the plot. 

3) Plotting the two-intercepts model
    i. Create a plot for each subject, (similar to part 3 in Exercise 1), this time also indicating the fitted value for each of the subjects for each for the scenarios (hint use `fixef` to get the "grand effects" for each gender and `ranef` to get the subject- and scenario-specific effects)

```{r}
# removing all NA values from the dataset
politeness_no_na <- na.omit(politeness)

#calculating the fitted values
politeness_no_na$predicted <- predict(lmer_gender_model_both)

#plotting observations for each subject with fitted values for each of the subjects for each of the scenarios
ggplot(politeness_no_na, aes(x = scenario, y = f0mn))+
  geom_point()+
  geom_point(aes(x = scenario, y = predicted), colour = "red")+
  facet_wrap(~subject)+
  theme_minimal()+
  ylab("Frequency in Hz")+
  ggtitle("Plot of observations for each subject with fitted values for each of the scenarios")
```


The red dots show the fitted values for our model. The black dots show the two measured frequencies in each scenario. 
    
## Exercise 3 - now with attitude

1) Carry on with the model with the two unique intercepts fitted (_scenario_ and _subject_).
  i. now build a model that has _attitude_ as a main effect besides _gender_

```{r}
attitude_model <- lmer(f0mn ~ gender + attitude + (1|subject) + (1|scenario), data = politeness_no_na)
summary(attitude_model)
```

  ii. make a separate model that besides the main effects of _attitude_ and _gender_ also include their interaction

```{r}
interaction_model <- lmer(f0mn ~ attitude*gender + (1|subject) + (1|scenario), data = politeness_no_na)
```

  iii. describe what the interaction term in the model says about Korean men's pitch when they are polite relative to Korean women's pitch when they are polite (you don't have to judge whether it is interesting)  

```{r}
summary(interaction_model)
```

```{r}
#making a plot that shows the interaction
pacman::p_load("effects")
plot(allEffects(interaction_model), multiline=TRUE, ci.style="bars")
```

The interactions tells us something about how much the effect of attitude (polite/informal) changes when gender goes from female to male. The interaction means that women's frequency is influenced more by the effect of attitude than males' frequency, since we see the slope is positive, thus meaning that the slope for when you go from informal to polite is steeper for women than men. However, we see that the error bars are overlapping quite a lot, there could be some uncertainty about whether this is a real effect. 

2) Compare the three models (1. gender as a main effect; 2. gender and attitude as main effects; 3. gender and attitude as main effects and the interaction between them. For all three models model unique intercepts for _subject_ and _scenario_) using residual variance, residual standard deviation and AIC. 

```{r}
model_names <- c("lmer_gender_model_both", "attitude_model", "interaction_model")

#finding residual variance
residual_variances <- c(var(resid(lmer_gender_model_both)), var(resid(attitude_model)), var(resid(interaction_model)))

#finding residual standard deviation
residual_sd <- c(sigma(lmer_gender_model_both), sigma(attitude_model), sigma(interaction_model))

#finding AIC
AIC <- c(AIC(lmer_gender_model_both), AIC(attitude_model), AIC(interaction_model))

#combining it all in a tibble
as.tibble(cbind(model_names, residual_variances, residual_sd, AIC))

```
The first model, with only gender as a fixed effect and random intercepts for subject and scenario, has the highest value on all three parameters. The interaction model has the lowest residual variance and lowest AIC value. However, model with gender and attitude as fixed effects (as well as random intercepts for subject and scenario) has the lowest residual standard deviation. But overall, there is not that big of a difference between the two models with attitude as a fixed effect.

3)  Choose the model that you think describe the data the best - and write a short report on the main findings based on this model. At least include the following:

**The following report is written as a study group - study group 4 with Sille, Caroline, Laura Paulsen and Louise**

The data set consists of frequency measurements of 7 men and 9 women. Their voice frequency was measured over 7 different scenarios, in two different attitudes - polite and informal. This means all participants went through 14 trials in total (except for a few missing values). Other variables include gender, hiss count and total duration of the specific trial. 

After running different models on the data, the following model was chosen to investigate the effect of gender and attitude on pitch. 

frequency ~ gender + attitude +  (1 | subject) +  (1 | scenario)

The dependent variable is the pitch frequency, the fixed effects include gender and attitude and subject and scenario are modelled as random intercepts. Since humans naturally have different pitch frequencies, and what we are interested in is how it is changed under certain circumstances, it is relevant to include random intercepts for each subject. If it was not included, the model would not take into account the repeated measurement design of the experiment. Furthermore, if you do not include random intercepts, you would overlook a very clear underlying effect in the data, thus having the risk of not interpreting the model properly. Therefore, including random intercepts for subject and scenario was found important.

This model was chosen since compared to a model with only gender as a fixed effect (and random intercepts for subject and scenario), the chosen model explained more variance. 
A more complex model including the interaction between gender and attitude could have been chosen, but using the _anova_ function we found the interaction to be insignificant. Thus a simpler model was preferred. 

After building the model a quantile-quantile plot of the chosen model was made testing for the assumption of normality of the residuals.

```{r}
qqnorm(resid(attitude_model), main = "Quantile Quantile plot for the residuals of the multilevel model")
qqline(resid(attitude_model))
```

By checking the plot, it was concluded that the model fulfilled the assumption, since most of the points are on the straight line, and we only see a small pattern of deviation at the end of the line. 

Investigating the coefficients, it was found that our intercept was 254.4, showing the average of the women’s pitch frequency in the informal condition. Gender significantly predicts frequency (β = -115.14, p< .001). Additionally, it was found that attitude significantly predicts frequency (β = -14.82, p< .001). This means that males generally have a lower frequency compared to women, and that changing attitude from informal to polite tends to result in a lower frequency when the other variables are held constant. 

We see that there is a higher variance (585.6) and std. dev. (24.20) for the second level effect subject compared to the second level effect of scenario where variance (106.7) and std. dev. (10.33) which means that there is a higher variability within _subjects_ compared to _scenarios_. 

The special thing shown by our model is that both Korean men and womens frequency gets lower in polite scenarios compared to informal scenarios, whereas in many other languages it is the opposite, when in polite scenarios the pitch gets higher for both men and women (Winter, 2013) ergo there are also cultural differences. 